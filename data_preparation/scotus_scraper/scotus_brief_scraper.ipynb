{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxuC_XpvWHXP",
    "outputId": "d51c69cd-d0d1-4215-f73c-fd9cd6a70871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwEssx7WNwn-"
   },
   "source": [
    "Docket numbers retrieved manually from the list maintained by findlaw.com. Only cases decided in 2018 and onward have their documents hosted on the Scotus website. Briefs from 2012 term onward are stored on scotusblog.com, although some of them are in a very messy format (i.e. a picture of a scanned document). Prior briefs used to be hosted by the ABA but they took them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Lw-88iQWBRha"
   },
   "outputs": [],
   "source": [
    "docket_nos_path = \"./docket_nos_valid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axmLJQzeBjES",
    "outputId": "03e4e86d-2d14-4080-a530-8787c0c63cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  docket_number  year url_list\n",
      "0         17-71  2018      NaN\n",
      "1       17-1676  2018      NaN\n",
      "2       18-5181  2018      NaN\n",
      "3        17-587  2018      NaN\n",
      "4       17-7894  2018      NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.DataFrame(columns=['docket_number', 'year', 'url_list'])\n",
    "\n",
    "current_year = None\n",
    "rows = [] \n",
    "\n",
    "with open(docket_nos_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for year-only lines. The year indicates when the decision was released (not when heard).\n",
    "        year_match = re.match(r'^<year>\\s+(\\d{4})$', line)\n",
    "        if year_match:\n",
    "            current_year = year_match.group(1)\n",
    "            continue\n",
    "\n",
    "        # Extract the docket number from regular lines\n",
    "        match = re.search(r'No\\.\\s+([\\w-]+)', line)\n",
    "        if match:\n",
    "            docket_number = match.group(1)\n",
    "            # Use the current year for this entry and add to the rows list\n",
    "            rows.append({'docket_number': docket_number, 'year': current_year})\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "kEqngQWVFFsZ",
    "outputId": "abc2278b-96be-42ea-c34a-6f4605e35941"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "\n",
    "# Setup WebDriver with headless option\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to process a single docket page\n",
    "def process_docket_page(docket_number):\n",
    "    url = f\"https://www.supremecourt.gov//docket/docketfiles/html/public/{docket_number}.html\"\n",
    "    pdf_urls = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page {url}: {str(e)}\")\n",
    "        return pdf_urls\n",
    "\n",
    "    try:\n",
    "        rows = driver.find_elements(By.TAG_NAME, 'tr')\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding table rows: {str(e)}\")\n",
    "        return pdf_urls\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            # Check if the row contains the text indicating a brief or document type\n",
    "            pattern = re.compile(r'^(?!.*\\b(motion|supplemental letter)\\b).*\\b(brief of|reply of|brief amicus curiae)\\b', re.IGNORECASE)\n",
    "            \n",
    "            if pattern.search(row.text):\n",
    "                # Find all 'a' elements in this row\n",
    "                # print(f\"Match found in row: {row.text}\")\n",
    "                next_td = row.find_element(By.XPATH, \"./following-sibling::tr[1]/td[2]\")\n",
    "                # print(f\"The next_td is {next_td.get_attribute('outerHTML')}\")\n",
    "                links = next_td.find_elements(By.TAG_NAME, 'a')\n",
    "                # print(f\"The links are: {links}\")\n",
    "                if len(links) > 0:\n",
    "                    for link in links:\n",
    "                        # print(f\"the link is {link.text}\")\n",
    "                        if link.text.strip().lower() == 'main document':\n",
    "                            pdf_urls.append(link.get_attribute('href'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pdf_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# List of docket numbers to process\n",
    "docket_numbers = df['docket_number'].tolist()\n",
    "\n",
    "docket_pdf_urls = {}\n",
    "\n",
    "# Try to do this directly with the dataframe rows, otherwise use a list.\n",
    "for idx, row in df.iterrows():\n",
    "    docket_number = row['docket_number']\n",
    "    pdf_urls = process_docket_page(docket_number)\n",
    "    docket_pdf_urls[docket_number] = pdf_urls\n",
    "    df.at[idx, 'url_list'] = pdf_urls\n",
    "    \n",
    "    time.sleep(random.uniform(1, 2)) # Hopefully avoid rate limits\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    }
   ],
   "source": [
    "print(len(docket_pdf_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',\n",
    "}\n",
    "\n",
    "num_briefs = 0\n",
    "\n",
    "for docket_number, urls_list in docket_pdf_urls.items():\n",
    "    num = 1\n",
    "    for url in urls_list:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(f'../data/scotus_pdfs/Docket{docket_number}_Brief{num:03}.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            num += 1\n",
    "            num_briefs += 1\n",
    "            # print(\"pdf saved successfully, I sleep now\")\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "        else:\n",
    "            print(f\"Failed to download PDF for docket number {docket_number}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You scraped 4377 briefs.\n"
     ]
    }
   ],
   "source": [
    "print(f\"You scraped {num_briefs} briefs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../data/scotus_briefs.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
